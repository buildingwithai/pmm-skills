# Iteration & Retrospective

**Purpose:** Build feedback loops and rapid iteration cycles for post-launch optimization.

**When to use:** Days 62-90+ (post-launch) and for continuous improvement.

**Output:** Iteration roadmap, launch retrospective, and growth playbook for next 90 days.

---

## Overview

Launch day is just the beginning. The real work happens in the weeks and months after launch as you learn from real users and iterate rapidly.

**Key principle:** Your launch plan is wrong. Not entirely wrong, but wrong enough that Week 1 feedback will reveal critical gaps.

**The iteration mindset:** Ship ‚Üí Measure ‚Üí Learn ‚Üí Iterate ‚Üí Repeat (2-week cycles).

---

## The Post-Launch Timeline

### Week 1 (Days 62-67): Data Collection

**Primary goal:** Gather as much feedback as possible

**Activities:**
- Monitor all metrics (AARRR dashboard)
- Read every support ticket
- Engage in social media comments
- Watch session recordings (Hotjar, FullStory)
- Note patterns in user behavior

**What NOT to do:**
- ‚ùå Make major product changes yet (need more data)
- ‚ùå Panic if metrics are below targets (Week 1 is noisy)
- ‚ùå Ignore negative feedback (it's the most valuable)

**Deliverable:** Week 1 findings document (see template below)

---

### Week 2-3 (Days 68-81): Rapid Iteration Sprint 1

**Primary goal:** Fix critical issues and low-hanging fruit

**Activities:**
- Prioritize fixes using ICE framework (Impact √ó Confidence √ó Ease)
- Ship daily improvements (bug fixes, copy tweaks, onboarding)
- A/B test highest-impact changes
- Re-engage churned users with improvements

**What to fix first:**
1. **Critical bugs** (breaks core functionality)
2. **Onboarding friction** (low activation rate)
3. **Messaging confusion** (users don't understand value)
4. **Missing features** (repeatedly requested)
5. **Performance issues** (slow load times)

**Deliverable:** Sprint 1 retrospective

---

### Week 4-5 (Days 82-95): Rapid Iteration Sprint 2

**Primary goal:** Optimize for activation and retention

**Activities:**
- Improve onboarding based on Week 2-3 learnings
- Launch retention campaigns (email, push, in-app)
- Test pricing/packaging variations
- Expand best-performing channels

**Deliverable:** Sprint 2 retrospective

---

### Week 6-8 (Days 96-110): Scaling Sprint

**Primary goal:** Scale what works, kill what doesn't

**Activities:**
- Increase budget for top channels
- Build growth loops (referral, content, SEO)
- Improve conversion funnel
- Plan next feature release

**Deliverable:** 30-day post-launch report + next 90-day growth plan

---

### Week 9-12 (Days 111-150): Growth Mode

**Primary goal:** Transition from launch to sustainable growth

**Activities:**
- Shift from "launch mode" to "growth mode"
- Build repeatable acquisition playbooks
- Focus on unit economics (LTV:CAC, payback)
- Plan next product/feature launch

**Deliverable:** Launch retrospective + lessons learned

---

## Feedback Collection Framework

### The Five Feedback Channels

**1. Quantitative Data (Analytics)**

**What to track:**
- AARRR metrics (see `07-metrics-optimization.md`)
- Funnel drop-offs
- Feature usage
- Cohort retention
- Session recordings

**Tools:**
- Mixpanel, Amplitude (event analytics)
- Hotjar, FullStory (session replay)
- Google Analytics (traffic)

**Cadence:** Real-time dashboards, daily reviews

---

**2. Qualitative Feedback (User Interviews)**

**Who to interview:**
- Activated users (understand what worked)
- Churned users (understand what failed)
- High-intent users who didn't convert (what held them back)
- Power users (what keeps them engaged)

**Questions to ask:**

**For activated users:**
- What problem were you trying to solve?
- What made you sign up?
- What was your "Aha moment"?
- What almost made you give up?
- What would make you recommend us?

**For churned users:**
- Why did you sign up?
- What did you expect vs what you got?
- What made you stop using the product?
- What would bring you back?
- What alternatives are you using now?

**Cadence:** 5-10 interviews in Week 1, 3-5/week ongoing

**Tools:**
- Calendly (scheduling)
- Zoom or Google Meet (video calls)
- Grain or Otter.ai (transcription)
- Dovetail or Airtable (analysis)

---

**3. Support Tickets (Customer Success)**

**What to track:**
- Volume (# of tickets)
- Categories (bug, feature request, how-to, billing)
- Time to first response
- Time to resolution
- Sentiment (positive, neutral, negative)

**Red flags:**
- High volume of same issue (broken feature or confusing UX)
- Long resolution times (support overwhelmed or complex issues)
- Negative sentiment trends (product not meeting expectations)

**Cadence:** Review daily summaries, deep dive weekly

**Tools:**
- Intercom, Zendesk, Help Scout (ticketing)
- Canny or ProductBoard (feature requests)

---

**4. Social Media & Community**

**Where to monitor:**
- Twitter/X mentions
- LinkedIn comments
- Reddit threads
- Product Hunt comments
- IndieHackers posts
- Slack/Discord communities

**What to look for:**
- Common praise (what's working)
- Common complaints (what's broken)
- Misconceptions (messaging unclear)
- Competitive mentions (alternatives being considered)

**Cadence:** Real-time during launch week, daily summaries after

**Tools:**
- Mention or Brand24 (social monitoring)
- F5Bot (Reddit keyword alerts)
- Google Alerts (news mentions)

---

**5. NPS & Surveys**

**When to survey:**
- Day 7: Short survey (how's it going?)
- Day 30: Full NPS survey (would you recommend?)
- Churn: Exit survey (why are you leaving?)

**NPS survey (3 questions):**
1. How likely are you to recommend [product] to a friend or colleague? (0-10 scale)
2. What's the primary reason for your score?
3. What can we improve?

**Day 7 survey (5 questions):**
1. Have you achieved what you wanted with [product]? (Yes/No/Partially)
2. How easy was it to get started? (1-5 scale)
3. What's been most valuable so far? (Open text)
4. What's been most frustrating? (Open text)
5. What's missing that you expected? (Open text)

**Cadence:** Automated triggered surveys

**Tools:**
- Typeform or Tally (beautiful surveys)
- Delighted (NPS focused)
- SurveyMonkey (traditional)

---

## The 2-Week Iteration Sprint

### Sprint Structure

**Duration:** 2 weeks (10 working days)

**Team:** Product + Engineering + Marketing + Design (as needed)

**Cadence:**
- **Monday Week 1:** Sprint planning (prioritize improvements)
- **Daily:** Standup (15 min, what shipped yesterday, what shipping today)
- **Friday Week 1:** Mid-sprint check-in (on track?)
- **Monday Week 2:** Continue execution
- **Friday Week 2:** Sprint retrospective + demo

---

### Sprint Planning (Day 1)

**Input:**
- Week 1 feedback summary
- AARRR metrics analysis
- Support ticket trends
- User interview insights
- Competitive intel

**Process:**
1. **Review feedback** (30 min) - What did we learn?
2. **Brainstorm improvements** (30 min) - What could we fix/improve?
3. **Prioritize using ICE** (30 min) - What should we fix first?
4. **Commit to sprint goals** (30 min) - What will we ship this sprint?

**Output:** Sprint backlog (5-10 items ranked by priority)

**ICE scoring template:**

| Improvement | Impact (1-10) | Confidence (1-10) | Ease (1-10) | ICE Score | Priority |
|-------------|---------------|-------------------|-------------|-----------|----------|
| Fix critical sign-up bug | 9 | 10 | 8 | 9.0 | 1 |
| Add onboarding video | 7 | 7 | 9 | 7.7 | 2 |
| Improve pricing page | 8 | 6 | 5 | 6.3 | 3 |
| Build referral program | 9 | 5 | 2 | 5.3 | 4 |

**Sprint capacity:** Be realistic. Assume 60-70% of planned work will ship (buffer for bugs, meetings, surprises).

---

### Daily Execution (Days 2-9)

**Daily standup (15 min, async OK):**
1. What did you ship yesterday?
2. What are you shipping today?
3. Any blockers?

**Shipping cadence:**
- Small fixes: Ship daily
- Medium changes: Ship every 2-3 days
- Large changes: Ship by end of sprint

**Communication:**
- #launch-team channel for updates
- Tag stakeholders on major decisions
- Share wins publicly (team morale)

---

### Sprint Retrospective (Day 10)

**Format:** 60-minute meeting with core team

**Agenda:**
1. **Review sprint goals** (10 min)
   - What did we commit to?
   - What did we ship?
   - What didn't we ship (and why)?

2. **Metrics review** (15 min)
   - Did metrics improve?
   - What moved the needle?
   - What didn't work?

3. **What went well** (15 min)
   - What should we keep doing?
   - What surprised us positively?
   - What wins should we celebrate?

4. **What could improve** (15 min)
   - What slowed us down?
   - What should we stop doing?
   - What should we do differently?

5. **Action items for next sprint** (5 min)
   - Process improvements
   - Tools needed
   - Blockers to remove

**Output:** Retrospective notes + action items for next sprint

---

## Common Post-Launch Patterns

### Pattern 1: The "Plateau" (Good)

**What it looks like:**
- Week 1: 1,000 sign-ups
- Week 2: 600 sign-ups
- Week 3: 500 sign-ups
- Week 4: 450 sign-ups
- Week 8: 400 sign-ups/week (stabilizes)

**Diagnosis:** Normal. Initial launch spike subsides, organic growth stabilizes.

**Action:** Focus on activation and retention. Build repeatable acquisition channels.

---

### Pattern 2: The "Hockey Stick" (Great)

**What it looks like:**
- Week 1: 1,000 sign-ups
- Week 2: 1,200 sign-ups
- Week 3: 1,500 sign-ups
- Week 4: 2,000 sign-ups
- Week 8: 3,500 sign-ups/week (accelerating)

**Diagnosis:** Viral growth, word-of-mouth working, or paid channel scaling well.

**Action:** Pour fuel on the fire. Scale what's working. Don't break what's growing.

---

### Pattern 3: The "Cliff" (Warning)

**What it looks like:**
- Week 1: 1,000 sign-ups
- Week 2: 300 sign-ups
- Week 3: 150 sign-ups
- Week 4: 80 sign-ups
- Week 8: 50 sign-ups/week (dying)

**Diagnosis:** Launch spike was artificial (hype, not value). Poor retention or weak channels.

**Action:** Deep dive on retention. User interviews. Revisit positioning/messaging. Consider pivot.

---

### Pattern 4: The "Rollercoaster" (Noisy)

**What it looks like:**
- Week 1: 1,000 sign-ups
- Week 2: 400 sign-ups
- Week 3: 800 sign-ups
- Week 4: 300 sign-ups
- Week 8: 600 sign-ups/week (volatile)

**Diagnosis:** Campaign-driven growth (spikes when you push, drops when you don't).

**Action:** Build organic channels (SEO, content, referrals). Reduce dependence on campaigns.

---

## Optimization Playbooks

### Playbook 1: Low Activation Rate (<25%)

**Symptoms:**
- Users sign up but don't complete onboarding
- High drop-off at specific steps
- Confused support tickets

**Root causes:**
- Onboarding too complex
- Value proposition unclear
- Product too buggy
- Wrong target audience signing up

**Fixes (prioritized by ICE):**

**High ICE (ship first):**
1. **Watch 20 session recordings** - Identify exact friction points
2. **Fix critical bugs** - Whatever breaks onboarding
3. **Simplify first step** - Remove fields, reduce clicks
4. **Add progress indicators** - Show "Step 2 of 4"
5. **Improve empty states** - Show demo data or examples

**Medium ICE (ship second):**
6. **Add onboarding video** - Show value in 60-90 seconds
7. **Send activation email** - "3 quick steps to get started"
8. **Build product tour** - Optional guided tour
9. **Personalize onboarding** - Different flows for different use cases
10. **A/B test onboarding flow** - Test variations

**Low ICE (ship later):**
11. **Build gamification** - Progress bars, achievements
12. **Add chat support** - Live help during onboarding
13. **Create onboarding webinar** - Weekly group onboarding

**Measure:** Track activation rate weekly. Target 10-15% improvement per sprint.

---

### Playbook 2: Poor Retention (<30% Day 7)

**Symptoms:**
- Users sign up, activate, then disappear
- Low DAU/MAU ratio
- Flat or declining cohort retention

**Root causes:**
- Product doesn't deliver ongoing value
- No habit formation
- Better alternatives exist
- Users achieved one-time goal

**Fixes (prioritized by ICE):**

**High ICE (ship first):**
1. **Email re-engagement campaign** - Day 3, Day 7, Day 14 emails
2. **Push notifications** - "You have a new message" (if mobile)
3. **Show progress/achievements** - "You've completed 5 tasks this week!"
4. **Add daily use case** - Give reason to return daily
5. **Fix core product bugs** - Eliminate frustration

**Medium ICE (ship second):**
6. **Build habit loops** - Daily streak, weekly summary
7. **Add social features** - Collaboration, comments, sharing
8. **Send weekly digest** - "Here's what happened this week"
9. **Improve core value prop** - Make product more valuable
10. **Add integrations** - Connect to tools users already use

**Low ICE (ship later):**
11. **Build community** - Forum, Slack, Discord
12. **Create content library** - Templates, tutorials
13. **Launch referral program** - Incentivize bringing friends

**Measure:** Track Day 7 and Day 30 retention by cohort. Target 5-10% improvement per sprint.

---

### Playbook 3: Low Conversion (<10% free‚Üípaid)

**Symptoms:**
- Users activate and stay, but don't pay
- Free tier meets all their needs
- Pricing page has high bounce rate

**Root causes:**
- Pricing too high
- Free tier too generous
- Paid tier value unclear
- Payment friction

**Fixes (prioritized by ICE):**

**High ICE (ship first):**
1. **Limit free tier** - Cap usage, features, or seats
2. **Add upgrade prompts** - When users hit limits
3. **Highlight paid benefits** - Show what they're missing
4. **Simplify checkout** - Remove fields, add payment methods
5. **Test pricing** - A/B test price points

**Medium ICE (ship second):**
6. **Add annual plan** - Discount for annual vs monthly
7. **Create usage-based pricing** - Align cost with value
8. **Build upgrade email sequence** - 7-day trial reminder, benefits
9. **Show ROI calculator** - "Save 10 hours/week = $500/month value"
10. **Add enterprise tier** - Higher price point for teams

**Low ICE (ship later):**
11. **Offer discounts** - First month 50% off
12. **Build sales team** - High-touch for enterprise
13. **Create annual campaign** - Black Friday, New Year deals

**Measure:** Track free‚Üípaid conversion weekly. Target 2-5% improvement per sprint.

---

## Launch Retrospective (Week 8-12)

### When to Run It

**Timing:** 30-60 days post-launch (once initial chaos settles)

**Duration:** 90-120 minutes

**Participants:** Full launch team (PMM, PM, Marketing, Sales, CS, Eng, Exec sponsor)

---

### Retrospective Format

**Pre-work (before meeting):**
- Compile metrics summary (AARRR dashboard)
- Collect feedback highlights
- Review sprint retrospectives
- Document major decisions and pivots

**Meeting agenda:**

**1. Metrics Review (20 min)**

Compare actuals vs targets:

| Metric | Target | Actual | Variance | Notes |
|--------|--------|--------|----------|-------|
| Week 1 sign-ups | 1,000 | 1,200 | +20% | PH launch exceeded expectations |
| Activation rate | 40% | 28% | -30% | Onboarding friction identified |
| Day 30 retention | 35% | 42% | +20% | Strong retention once activated |
| MRR | $10K | $7K | -30% | Conversion lower than expected |

**Key questions:**
- What metrics beat expectations (and why)?
- What metrics missed (and why)?
- What surprised us?

---

**2. What Went Well (20 min)**

**Prompt:** What should we keep doing?

**Example answers:**
- ‚úÖ Product Hunt launch drove 40% of Week 1 traffic
- ‚úÖ War room coordination kept team aligned
- ‚úÖ Pre-launch beta caught major bugs
- ‚úÖ Email sequences converted well (35% open rate)
- ‚úÖ Sales team was well-prepared (great battlecards)

**Output:** Document what worked for future launches

---

**3. What Could Improve (20 min)**

**Prompt:** What should we do differently next time?

**Example answers:**
- ‚ùå Onboarding was too complex (7 steps ‚Üí should be 3)
- ‚ùå Pricing page was confusing (too many tiers)
- ‚ùå Messaging didn't resonate with developers (focused on marketers)
- ‚ùå Waited too long to start SEO (should start 6 months before launch)
- ‚ùå Didn't allocate enough CS support (overwhelmed Week 1)

**Output:** Action items to prevent repeated mistakes

---

**4. Customer Insights (20 min)**

**Prompt:** What did we learn about our customers?

**Example insights:**
- üí° Primary users are not who we expected (marketing ops, not CMOs)
- üí° Main use case is X, not Y (shifted positioning)
- üí° Users need integration with Salesforce (prioritize roadmap)
- üí° Onboarding confusion around "workspaces" concept (rename)
- üí° Competitors are Notion and Airtable, not expected tools

**Output:** Update personas, positioning, messaging based on learnings

---

**5. Process Improvements (15 min)**

**Prompt:** How can we improve our launch process?

**Example improvements:**
- üìù Add 2 more weeks for content creation (rushed)
- üìù Start sales enablement earlier (4 weeks vs 2 weeks before launch)
- üìù Run full launch dry run (caught issues day-of)
- üìù Build launch checklist (use for next launch)
- üìù Weekly stakeholder updates (not daily, too frequent)

**Output:** Update launch playbooks and templates

---

**6. Celebrate Wins (5 min)**

**Important:** Don't skip this. Acknowledge hard work, even if results were mixed.

- üéâ Thank team members publicly
- üéâ Share favorite user testimonials
- üéâ Highlight individual contributions
- üéâ Order team dinner or send gift cards

---

### Retrospective Deliverable

**Launch Retrospective Document** (share with stakeholders):

**Section 1: Executive Summary**
- Launch date and tier
- Key metrics (vs targets)
- Major wins
- Major learnings
- Next steps

**Section 2: Detailed Metrics**
- AARRR breakdown
- Channel performance
- Cohort analysis
- Financial metrics (if applicable)

**Section 3: What Worked**
- Tactics that succeeded
- Team strengths
- Unexpected wins

**Section 4: What Didn't Work**
- Tactics that failed
- Bottlenecks encountered
- Missed opportunities

**Section 5: Customer Insights**
- Who actually uses the product
- How they use it
- What they value most
- What's missing

**Section 6: Recommendations**
- Product roadmap updates
- Messaging/positioning changes
- Channel strategy shifts
- Process improvements for next launch

**Section 7: Next 90 Days**
- Growth plan
- Key initiatives
- Resource needs
- Success metrics

---

## Transitioning to Growth Mode

### From Launch to Growth (Week 9+)

**Launch mode characteristics:**
- High intensity, all-hands effort
- War room coordination
- Daily check-ins
- Rapid iteration

**Growth mode characteristics:**
- Sustainable pace
- Repeatable playbooks
- Weekly check-ins
- Systematic optimization

**How to transition:**

**Week 9-10:**
- Wind down war room (move to weekly meetings)
- Document launch playbooks
- Hand off from PMM to growth team
- Shift focus from "launch tactics" to "growth loops"

**Week 11-12:**
- Build repeatable acquisition channels
- Optimize conversion funnels
- Focus on unit economics
- Plan next product/feature launch

---

### Building Growth Loops

**Growth loop = Self-reinforcing cycle that drives acquisition**

**Example loops:**

**Referral loop:**
```
New user signs up ‚Üí Invites teammates ‚Üí Teammates sign up ‚Üí Invite more teammates ‚Üí ...
```

**Content loop:**
```
Publish valuable content ‚Üí Ranks on Google ‚Üí Drives organic traffic ‚Üí New users sign up ‚Üí Share content ‚Üí ...
```

**Product loop:**
```
User creates project ‚Üí Shares with client ‚Üí Client signs up ‚Üí Creates projects ‚Üí ...
```

**Network effect loop:**
```
User joins platform ‚Üí More users = more value ‚Üí Platform becomes more valuable ‚Üí Attracts more users ‚Üí ...
```

**How to build loops:**
1. Map your existing user journey
2. Identify potential loop points (where sharing/inviting makes sense)
3. Build mechanics to encourage loop behavior
4. Measure loop strength (K-factor, cycle time)
5. Optimize for faster cycles

---

## Common Post-Launch Mistakes

### Mistake 1: Stopping After Launch

**Problem:** Treating launch as the finish line instead of the starting line.

**Impact:** Momentum dies, metrics plateau, team moves on.

**Fix:** Commit to 90 days of post-launch optimization before declaring success/failure.

---

### Mistake 2: Ignoring Negative Feedback

**Problem:** Focusing only on positive testimonials, dismissing criticism.

**Impact:** Miss critical insights, product doesn't improve.

**Fix:** Actively seek out negative feedback. Call churned users. Read 1-star reviews.

---

### Mistake 3: Changing Everything at Once

**Problem:** Shipping 10 changes in one sprint, can't tell what worked.

**Impact:** No learning, metrics don't improve, wasted effort.

**Fix:** Ship incrementally. A/B test major changes. Measure before and after.

---

### Mistake 4: Over-Optimizing Vanity Metrics

**Problem:** Chasing sign-ups or traffic instead of activation/retention.

**Impact:** Lots of users, but they churn. Unit economics don't work.

**Fix:** Focus on activation and retention first. Acquisition last.

---

### Mistake 5: No Retrospective

**Problem:** Moving to next launch without documenting learnings.

**Impact:** Repeat same mistakes, team doesn't improve.

**Fix:** Mandatory retrospective 30-60 days post-launch. Document and share.

---

## Iteration Checklist

Use this to ensure you're iterating effectively:

### Week 1-2 Post-Launch
- [ ] Collect feedback from all 5 channels (analytics, interviews, support, social, surveys)
- [ ] Watch 20+ session recordings
- [ ] Interview 5-10 activated users
- [ ] Interview 5-10 churned users
- [ ] Review all support tickets for patterns
- [ ] Identify top 3 bottlenecks in AARRR funnel
- [ ] Run Sprint 1 (prioritize using ICE framework)
- [ ] Ship 5-10 improvements
- [ ] Document what worked and what didn't

### Week 3-4 Post-Launch
- [ ] A/B test highest-impact changes
- [ ] Re-engage churned users with improvements
- [ ] Expand best-performing channels
- [ ] Run Sprint 2
- [ ] Ship 5-10 more improvements
- [ ] Review metrics vs Week 1 (are we improving?)

### Week 5-8 Post-Launch
- [ ] Scale what works, kill what doesn't
- [ ] Increase budget for top channels
- [ ] Build growth loops (referral, content, product)
- [ ] Test pricing/packaging variations
- [ ] Plan next feature release

### Week 9-12 Post-Launch
- [ ] Run full launch retrospective
- [ ] Document lessons learned
- [ ] Update launch playbooks
- [ ] Create next 90-day growth plan
- [ ] Transition from launch mode to growth mode
- [ ] Celebrate team success

---

## Next Steps

**After completing iteration cycles:**

1. **Update positioning/messaging** based on customer insights
   - Use `positioning` skill to refine
   - Use `messaging` skill to update value props

2. **Plan next launch** (feature or product)
   - Apply learnings from this launch
   - Use updated playbooks and templates

3. **Build growth team** and transition ownership
   - From launch team to growth team
   - Document handoff

4. **Scale successful channels** systematically
   - See `05-multi-channel-tactics.md`

**Related guides:**
- `07-metrics-optimization.md` - AARRR metrics deep dive
- `06-launch-day-playbook.md` - Real-time monitoring
- `02-90-day-roadmap.md` - Full launch timeline
- `01-launch-tier-framework.md` - Resource allocation

**Remember:** Launches don't fail in Week 1. They fail in Week 2-12 when teams stop iterating.
